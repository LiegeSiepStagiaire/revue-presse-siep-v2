
import streamlit as st
import feedparser
from datetime import datetime, timedelta
import pandas as pd

st.set_page_config(page_title="Revue de presse SIEP", page_icon="üì∞", layout="wide")

rubriques = {'Travail et Insertion Socio-Professionnelle': ['emploi', 'recherche d‚Äôemploi', 'l√©gislation du travail', 'contrat', 'job √©tudiant', 'insertion socio-professionnelle', 'CISP', 'ann√©e citoyenne', 'volontariat', 'r√©daction de CV'], 'Enseignement de plein exercice : secondaire et sup√©rieur': ['√©tudes', 'enseignement secondaire', 'enseignement sup√©rieur', 'enseignement qualifiant', 'd√©cret', 'structure scolaire', 'organisation des √©tudes', '√©tablissement scolaire', 'droit scolaire', 'exclusion scolaire', 'recours scolaire', 'acc√®s aux √©tudes', 'co√ªt des √©tudes', 'bourse d‚Äô√©tude', 'pr√™t d‚Äô√©tude', 'CPMS', '√©cole de devoirs', 'rem√©diation', 'm√©thode de travail', 'aide √† la r√©ussite', 'tutorat', 'DASPA', 'certification', 'choix d‚Äô√©tudes', 'ann√©e pr√©paratoire', 'journ√©e portes ouvertes', 'passerelle', 'valorisation des acquis (VAE)'], 'Formation': ['formation en alternance', 'CEFA', 'IFAPME', 'EFP', 'jury', 'enseignement √† distance', 'horaire r√©duit', 'alphab√©tisation', 'promotion sociale', 'formation demandeur d‚Äôemploi', 'FOREM', 'ACTIRIS', 'centre de comp√©tence', 'validation des comp√©tences'], 'Protection sociale / aide aux personnes': ['ch√¥mage', 'mutuelle', 'aide sociale', 'revenu d‚Äôint√©gration sociale (RIS)', 'allocations familiales', 'financement des √©tudes', 'aide √† la jeunesse'], 'Vie familiale et affective': ['sexualit√©', 'planning familial', '√©galit√© des genres', 'animation GDBD', 'charte √©galit√©'], 'Qualit√© de vie': ['sant√©', 'consommation', 'harc√®lement', 'sensibilisation', 'logement interg√©n√©rationnel', 'kot √©tudiant', 'contrat de bail', 'transport'], 'Loisirs / vacances': ['sport', 'stage vacances', 'formation animateur', 'centre d‚Äôh√©bergement', 'centre de rencontre'], 'International': ['projet international', 's√©jour linguistique', 'stage √† l‚Äô√©tranger', 'apprendre les langues', 'bourse internationale', 'test de langue', 'niveau CECRL', 'mobilit√© europ√©enne'], '√ätre acteur dans la soci√©t√© / Institutions et justice': ['citoyennet√©', 'droits', 'devoirs', 'engagement', 'implication politique', 'd√©mocratie', 'droit √† l‚Äôimage', 'r√©seaux sociaux', 'nationalit√©', 'institutions belges', 'institutions europ√©ennes', 'droits humains', 'partis politiques', 'participation des jeunes', 'mouvements philosophiques', 'police', 'justice', 'groupe de pression'], 'Les M√©tiers': ['orientation m√©tier', 'projet de vie', 'connaissance de soi', 'information m√©tier', 'exploration', 'rencontre professionnelle']}

sources_fiables = [
    "rtbf.be", "lesoir.be", "lalibre.be", "lecho.be", "sudpresse.be",
    "forem.be", "actiris.be", "siep.be", "enseignement.be",
    "enseignement.catholique.be", "cfwb.be", "socialsecurity.be",
    "emploi.belgique.be", "bx1.be", "guide-social.be", "dhnet.be", "jobat.be",
    "lanouvellegazette.be", "references.lesoir.be", "emploi.wallonie.be",
    "enseignement.cfwb.be", "monorientation.be", "volontariat.be",
    "jeminforme.be", "citedesmetiers.be", "onem.be", "solidaris.be",
    "rtlinfo.be", "telesambre.be", "televesdre.be", "vivacite.be",
    "not√©l√©.be", "canalzoom.be", "namurinfo.be", "journal-lavenir.be",
    "lejournaldujeudi.be", "proximus.be", "metrolibre.be"
]

mois_fr = {
    "Jan": "janvier", "Feb": "f√©vrier", "Mar": "mars", "Apr": "avril",
    "May": "mai", "Jun": "juin", "Jul": "juillet", "Aug": "ao√ªt",
    "Sep": "septembre", "Oct": "octobre", "Nov": "novembre", "Dec": "d√©cembre"
}

jours_fr = {
    "Mon": "lundi", "Tue": "mardi", "Wed": "mercredi", "Thu": "jeudi",
    "Fri": "vendredi", "Sat": "samedi", "Sun": "dimanche"
}

def format_date_francaise(date_str):
    try:
        parts = date_str.split(" ")
        jour = jours_fr.get(parts[0][:3], parts[0])
        num = parts[1]
        mois = mois_fr.get(parts[2], parts[2])
        annee = parts[3]
        heure = parts[4][:5].replace(":", "h")
        return f"{jour} {num} {mois} {annee} √† {heure}"
    except:
        return date_str

def create_rss_url(keyword, start_date, end_date):
    base_url = "https://news.google.com/rss/search?"
    query = f"q={keyword.replace(' ', '+')}+after:{start_date}+before:{end_date}"
    params = "&hl=fr&gl=BE&ceid=BE:fr"
    return base_url + query + params

def get_articles(rss_url, keyword, sources, accept_all=True):
    feed = feedparser.parse(rss_url)
    articles = []
    for entry in feed.entries:
        link = entry.link
        title = entry.title
        summary = entry.get("summary", "")
        if accept_all and (not geo_sources or any(src in link for src in geo_sources) or any(src in link for src in sources)):
            if keyword.lower() in title.lower() or keyword.lower() in summary.lower():
                articles.append({
                    "title": title,
                    "link": link,
                    "date": format_date_francaise(entry.published if 'published' in entry else ""),
                    "keyword": keyword
                })
    return articles

st.title("Revue de presse üìö - SIEP Li√®ge")

rubrique = st.selectbox("Rubrique", list(rubriques.keys()))
col1, col2 = st.columns(2)
start_date = col1.date_input("üìÖ Date de d√©but", datetime.today() - timedelta(days=30))
end_date = col2.date_input("üìÖ Date de fin", datetime.today())

custom_sources_input = st.text_input("üîó Ajouter des sites web suppl√©mentaires (ex: https://mon-site.be), s√©par√©s par des virgules :")
custom_sources = [url.strip().replace("https://", "").replace("http://", "").strip("/") for url in custom_sources_input.split(",") if url.strip()]

custom_keyword = st.text_input("üìù (Optionnel) Rechercher un mot-cl√© personnalis√© en plus de ceux de la rubrique :")

# La case pour "sources non v√©rifi√©es" est supprim√©e => toujours True
zone_geo = st.selectbox("Zone g√©ographique cibl√©e", list(zones_sources_named.keys()))
accept_all = True
geo_sources = zones_sources_named[zone_geo]

if 'article_history' not in st.session_state:
    st.session_state.article_history = []
    st.session_state.deleted_stack = []

if st.button("üîç Rechercher"):
    total_articles = []
    mots_capt√©s = set()
    keywords_to_search = [custom_keyword] if custom_keyword else rubriques[rubrique].copy()
    if custom_keyword:
        keywords_to_search.append(custom_keyword)

    for keyword in keywords_to_search:
        url = create_rss_url(keyword, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
        articles = get_articles(url, keyword, sources_fiables + custom_sources, accept_all)
        if articles:
            mots_capt√©s.add(keyword)
        total_articles.extend(articles)

    seen = set()
    filtered_articles = []
    for article in total_articles:
        key = (article['title'], article['link'])
        if key not in seen:
            seen.add(key)
            filtered_articles.append(article)

    st.session_state.article_history = filtered_articles
    st.session_state.deleted_stack = []

    if mots_capt√©s:
        st.success("Mots-cl√©s ayant donn√© des r√©sultats : " + ", ".join(sorted(mots_capt√©s)))
    else:
        st.info("Aucun mot-cl√© n'a donn√© de r√©sultat pour cette p√©riode.")

if st.session_state.article_history or st.session_state.article_history == []:
    search_filter = st.text_input("üîç Filtrer les articles par mot-cl√© dans le titre :", "")
    for i, article in enumerate(st.session_state.article_history):
        if search_filter.lower() in article['title'].lower():
            cols = st.columns([5, 1, 1])
            cols[0].markdown(f"**{article['title']}**  \n_{article['date']}_  \nüîó [Lire l'article]({article['link']})")
            if cols[1].button("üóëÔ∏è Supprimer", key=f"delete_{i}"):
                st.session_state.deleted_stack.append(article)
                st.session_state.article_history.pop(i)
                st.rerun()

    colA, colB, colC = st.columns(3)
    if colA.button("‚Ü©Ô∏è Revenir en arri√®re", disabled=not st.session_state.deleted_stack):
        last_deleted = st.session_state.deleted_stack.pop()
        st.session_state.article_history.append(last_deleted)
        st.rerun()

    df_export = pd.DataFrame(st.session_state.article_history)
    csv = df_export.to_csv(index=False).encode('utf-8')
    colC.download_button("‚¨áÔ∏è Exporter en CSV", data=csv, file_name="revue_presse_siep.csv", mime="text/csv")

if not st.session_state.article_history:
    st.warning("Aucun article pertinent trouv√© pour cette rubrique et cette p√©riode.")
